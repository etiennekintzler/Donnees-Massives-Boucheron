---
title: "DO_KINT"
output: pdf_document
---

#Perform approximate nearest neighbor analysis
## Introduction
Pour traiter le problème de cluster ou k-plus proches voisins, on doit comparer tous les pairs d'observations possibles dans les données, ainsi le coût de calcul va exploser très vite en fontion de taille de données et la dimension du problème. Pour remédier ce problème, on peut utiliser l'algorithme LSH qui utilise des fonctions min-hashing randomnisées pour estimer le cofficient Jaccard-similarité entre des documents afin de trouver des pairs de documents tels que leur coefficient de similarité est supérieur à un niveau donné. 

## Model LSH pour text mining

### Fonction min-hashing

D'abord, on rappelle la définition de la similarité-Jaccard entre deux documents $A$ et $B$:  
$JS(A,B) =\frac{|A\bigcap B|}{|A \bigcup B|}$  
Fonction min-hashing $m(. )$ va rendre le rang de premier terme appartient au document selon l'ordre générée par cette fonction hashing. On définit une variable aléatoire $x(A,B)$ telle que :  
$x(A,B) = 1$ si $m(A) = m(B)$   
$x(A,B) =0$  sinon.   
Par conséquence, la variable aléatoire $x(A,B)$ suit la loi B(JS(A,B)). Donc, on peut estmier la similarité de Jaccard, en générant des observations iid de cette loi $B(JS(A,B))$ par des fonctions min-hashing et moyennant des valuers.  

### Locality sensitive hashing (LSH)
Dans le cardre de text mining, on utilise la fonction hashing par bloc des singatures générées par les fonctions min-hashing, comme la famille de LSH. Effectivement, si l'on sonsidère $n$ signatures avec $b$ blocs de taille $r$ ou bien : $n= b*r$, la fonction LSH considère que deux documents forment un pair de candidat s'ils sont coïncides au moins pour un blocs. La probabilité telle que une pair de documents est une paire de candidat est ainsi donnée par :  
$P(JS(A,B)) = 1 - ( 1-JS(A,B)^{r} )^ b$  


```{r }
plot(seq(0,1,0.01),   1- ( 1 -  seq(0,1,0.01)**10 )**50 , type = "l", xlab = "JS(A,B)", ylab = "P(JS(A,B)) ", col="red" )

```

Cette  probabilité possède une forme S-courbe, croissante $P(0) = 0$ et $P(1) =1$.  



On peut ajuster des valeurs de $b$ et $r$ pour manipuler le seuil de $JS(A,B)$ tel que $A$ et $B$ sont un pair de candidats avec une grande probabilité.

## Implémentation sous R 


```{r }
library("textreuse")
library("dplyr")
library("tm")
library("magrittr")
library("foreach")
library("SnowballC")
library("igraph")
```

```{r }
lsh_threshold(h = 200, b = 25)
```

Générer des fonctions min-hashing 

```{r }
minhash <- minhash_generator(n = 200)
```

Construire le corpus de TextReuseCorpus pour utiliser LSH

```{r, warning=FALSE, eval= FALSE}

dir <- "~/Desktop/projet_bigdata_P7/dta"

Corpus <- VCorpus(DirSource(dir, encoding= "UTF-8"), 
                    readerControl=list(language= "en") )   

Corpus <- Corpus %>%
        tm_map(content_transformer(tolower),lazy=TRUE) %>%
        tm_map(removeWords,stopwords("english"), lazy=TRUE) %>%
        tm_map(stemDocument,lazy=TRUE) %>% 
        tm_map(removePunctuation,lazy=TRUE) %>% 
        tm_map(stripWhitespace,lazy=TRUE) %>%
        tm_map(removeNumbers,lazy=TRUE)
```


```{r, warning=FALSE, eval= FALSE}

barCorpus <- foreach (doc=Corpus, .combine="c") %do% TextReuseCorpus(text=doc$content, 
                  tokenizer = tokenize_words,
                  progress = TRUE,
                  keep_tokens = TRUE,
                  keep_text = TRUE, 
                  skip_short = TRUE)

```


```{r, warning=FALSE}
dir <- "~/Desktop/projet_bigdata_P7/dta"
corpus <- TextReuseCorpus(dir = dir , minhash_func = minhash, skip_short = TRUE, n=10)
```

Construire les buckets. Le nombre de bands est divisé par le nombre de fonctions min-hashing.

```{r}
####CONSTRUIRE LES BUCKETS 

buckets <- lsh(corpus, bands = 25, progress = FALSE)
```



Trouver de pairs de candidat liées avec un document précise.
```{r}
baxter_matches <- lsh_query(buckets, "100")
baxter_matches
```


Trouver tous les pairs de candidat
```{r}
#returns all potential pairs of matches.
candidates <- lsh_candidates(buckets)
candidates
```



Comparaison les pairs de candidat avec la similarité de Jaccard
```{r}
#lsh_compare() to apply a similarity function to the candidate pairs of documents
lsh_compare(candidates, corpus, jaccard_similarity, progress = FALSE)
```




```{r, echo=FALSE}
a= rep(0,2*nrow(candidates) )
a[2*c(1:nrow(candidates))] = as.integer(candidates$b)
a[-2*c(1:nrow(candidates))] = as.integer(candidates$a)
```



```{r}
gr<-graph( edges=a+1,  directed=F ) 
gr2=delete.vertices(gr,which(degree(gr)<4))
plot(gr2, vertex.size=4, vertex.label=NA, layout= layout.lgl) 

```


Cherher des composants (clusters ) dans ce graphe 

```{r}
clusters <- clusters(gr2)
plot(gr2, vertex.color = clusters$membership, vertex.size=4, vertex.label=NA, layout= layout.lgl)
```


Pour chaque cluster, on cherche des mots les plus fréquents et les affiche 


```{r, echo=FALSE, warning=FALSE}
library(tm)
enronp <- "~/Desktop/projet_bigdata_P7/dta"
Venronp <- VCorpus(DirSource(enronp, encoding= "UTF-8",recursive=T),
                   readerControl=list(language= "en") ) 

Venronp <- Venronp %>%
        tm_map(content_transformer(tolower),lazy=TRUE) %>%
        tm_map(removeWords,stopwords("english"), lazy=TRUE) %>%
        tm_map(stemDocument,lazy=TRUE) %>% 
        tm_map(removePunctuation,lazy=TRUE) %>% 
        tm_map(stripWhitespace,lazy=TRUE) %>%
        tm_map(removeNumbers,lazy=TRUE)
```



```{r}
for (i in 1: (clusters$no)) {
  print(paste(" Pour le cluster ", i, "les mots specifiques sont: " ))
  doc = which(clusters$membership== i)
  corpus_cluster = Venronp[doc]
  
  mots = findFreqTerms(TermDocumentMatrix(corpus_cluster) , lowfreq = clusters$csize[i]/2, highfreq = Inf)
  print(mots ) 
  print("==============================================================")}
```



