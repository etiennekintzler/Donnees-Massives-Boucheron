---
title: |
    | Projet en Traitement des données massives 
    | (S. Boucheron)
author:  "Minh Tuan Do, Etienne Kintzler"
output: pdf_document
header-includes:
  - \usepackage{dsfont, amsmath, amssymb, bm}
---
\section{Introduction}
Le projet a pour objectif d'utiliser deux méthodes vues en cours, le Locally 
Sensitive Hashing et le Latent Dirichlet Allocation à une base de donnée 
célèbre, les mails de l'entreprise Américaine Enron.\\
Les languages de programmation utilisées sont Python pour une des tâches de 
pré-traitement et R pour le reste du pré-traitement ainsi que pour l'application
des méthodes vues en cours.

\subsection{Description de la base de donnée Enron}
La base de données est composé d'environ 500 000 messages envoyées/reçues par
environ 150 utilisateurs (principalement des managers). La base de données est
disponible à l'adresse suivante :\\
https://www.cs.cmu.edu/~./enron/

\subsection{Construction du corpus et shingling}
  \subsubsection{Pré-traitement et construction du corpus}
Nous utilisons un script python afin de sélectionner les mails des sous-dossier
"sent" uniquement. Le script permet aussi de supprimer les élements propre à la 
structure des mails comme :"To","From","Subject:" etc.

La construction du corpus se fait à l'aide du package \texttt{tm}.

```{r,echo=T, message=FALSE, results='hide'}
plyr::laply(.data=c("ggplot2","tm","topicmodels","filehash", "SnowballC","slam","textreuse"),
                     .fun=require,
                     character.only=TRUE,
                     quietly=TRUE,
                     warn.conflicts=FALSE)

#Repertoire dans lequel se trouve le dossier avec les mails
setwd("~/R/projet_boucheron/")
enronpath <- paste(getwd(),"sub",sep="/") # dir of enron data
```

```{r}
setwd("~/R/projet_boucheron/")
enronpath <- paste(getwd(),"sub",sep="/") # dir of enron data
enron    <- VCorpus(DirSource(enronpath, encoding= "UTF-8"), 
                    readerControl=list(language= "en"))    
```

\section{LSH}

\section{Latent Dirichlet Allocation}
Le package \texttt{topicmodels} est utilisé pour réaliser le LDA.

\subsection{Formalisation}
Un \textit{mot} est définit comme un élement do vocabulaire indexé par $\{1,..,|\mathcal{A}|\}$. Des indices sont utilisées pour désigner les composantes d'un mot. Par exemple le \textit{v}-ème mot du dictionnaire est représenté par un vecteur de taille $|\mathcal{A}|$ telle que $w^v=1$ et $w^u=0,\forall u\neq v$.

Un \textit{document} est une N-uplets de mots dénoté $\bm{w}=(w_1,w_2,...,w_N)$ où $w_n$ est le n-ème mot de la séquence.

Un \textit{corpus} est un ensemble de $M$ documents dénoté: $\bm{D}=\{ \bm{w}_1,\bm{w}_2,...,\bm{w}_M \}$ 

\subsection{Présentation du modèle}
Les PGD pour chacun des documents \textbf{w} du corpus \textit{\textbf{D}} est le suivant:

\begin{enumerate}
  \item Choisir $N\sim\mathcal{P}(\xi)$
  \item Choisir $\theta\sim\text{Dir}(\alpha)$
  \item Pour chacun des $N$ mot $w_n$:
  \begin{enumerate}
    \item Choisir un topic $z_n\sim\mathcal{M}(\theta)$
    \item Choisir un mot $w_n$ a partir de $p(w_n|z_n,\beta)$, une probabilité d'une loi multinomiale conditionné par le topic $z_n$
  \end{enumerate}
\end{enumerate}
